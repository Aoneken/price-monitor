"""
Pesta√±a 2: Scraping
Interfaz para iniciar el proceso de scraping con progreso en tiempo real
"""
import streamlit as st
from datetime import datetime, timedelta
from database.db_manager import get_db
from scrapers.orchestrator import ScrapingOrchestrator

st.set_page_config(page_title="Scraping", page_icon="üîç", layout="wide")

st.title("üîç Monitoreo de Precios")
st.markdown("Ejecuta el scraping de precios para tus establecimientos.")

db = get_db()

# === SECCI√ìN 1: CONFIGURACI√ìN DE LA TAREA ===
st.header("‚öôÔ∏è Configuraci√≥n del Scraping")

establecimientos = db.get_all_establecimientos()

if not establecimientos:
    st.warning("‚ö†Ô∏è No hay establecimientos registrados. Ve a la pesta√±a 'Establecimientos' para crear uno.")
    st.stop()

# Selector de establecimiento
nombres_establecimientos = {
    est['nombre_personalizado']: est['id_establecimiento']
    for est in establecimientos
}

col_config1, col_config2 = st.columns(2)

with col_config1:
    establecimiento_seleccionado = st.selectbox(
        "Seleccionar Establecimiento",
        options=list(nombres_establecimientos.keys())
    )
    id_establecimiento = nombres_establecimientos[establecimiento_seleccionado]
    
    # Verificar URLs activas
    urls_activas = db.get_urls_activas_by_establecimiento(id_establecimiento)
    st.info(f"üìä URLs activas para '{establecimiento_seleccionado}': **{len(urls_activas)}**")
    
    if urls_activas:
        for url in urls_activas:
            st.caption(f"‚Ä¢ {url['plataforma']}")

with col_config2:
    # Selector de rango de fechas
    st.subheader("Rango de Fechas")
    
    fecha_inicio = st.date_input(
        "Fecha de Inicio",
        value=datetime.now().date(),
        help="Primera noche a monitorear"
    )
    
    fecha_fin = st.date_input(
        "Fecha de Fin",
        value=(datetime.now() + timedelta(days=30)).date(),
        help="√öltima noche a monitorear"
    )
    
    # Calcular d√≠as
    if fecha_fin >= fecha_inicio:
        dias_total = (fecha_fin - fecha_inicio).days + 1
        st.metric("D√≠as a Scrapear", dias_total)
    else:
        st.error("‚ùå La fecha de fin debe ser posterior a la fecha de inicio")

st.divider()

# === SECCI√ìN 2: EJECUCI√ìN Y PROGRESO ===
if not urls_activas:
    st.error("‚ùå No hay URLs activas para este establecimiento. Agrega URLs en la pesta√±a 'Establecimientos'.")
    st.stop()

if fecha_fin < fecha_inicio:
    st.error("‚ùå Rango de fechas inv√°lido.")
    st.stop()

# Bot√≥n de inicio
iniciar_scraping = st.button("üöÄ INICIAR MONITOREO", type="primary", use_container_width=True)

if iniciar_scraping:
    st.header("üìä Progreso del Scraping")
    
    # Contenedores para actualizaci√≥n en tiempo real
    contenedor_estado = st.empty()
    contenedor_progreso = st.empty()
    contenedor_tabla = st.empty()
    contenedor_log = st.empty()
    
    # Lista para acumular resultados
    resultados_tabla = []
    
    # Funci√≥n callback para actualizar UI
    def callback_progreso(mensaje, progreso, resultado):
        if mensaje:
            contenedor_estado.info(f"üîÑ {mensaje}")
        
        if progreso is not None:
            contenedor_progreso.progress(progreso)
        
        if resultado:
            # Agregar resultado a la tabla
            resultados_tabla.append({
                "Plataforma": resultado.plataforma,
                "Fecha": resultado.fecha_noche.strftime('%Y-%m-%d'),
                "Precio": f"${resultado.precio:.2f}" if resultado.precio > 0 else "No disponible",
                "Noches": resultado.noches if resultado.noches > 0 else "-",
                "Estado": "‚úÖ OK" if not resultado.error else f"‚ùå {resultado.error[:30]}..."
            })
            
            # Actualizar tabla (mostrar √∫ltimos 20)
            contenedor_tabla.dataframe(
                resultados_tabla[-20:],
                use_container_width=True,
                hide_index=True
            )
    
    # Ejecutar scraping
    try:
        orchestrator = ScrapingOrchestrator(callback_progreso)
        
        fecha_inicio_dt = datetime.combine(fecha_inicio, datetime.min.time())
        fecha_fin_dt = datetime.combine(fecha_fin, datetime.min.time())
        
        resultados = orchestrator.ejecutar(
            id_establecimiento,
            fecha_inicio_dt,
            fecha_fin_dt
        )
        
        # Mostrar resumen final
        contenedor_estado.success("‚úÖ Scraping completado exitosamente!")
        contenedor_progreso.progress(1.0)
        
        st.divider()
        st.header("üìà Resumen de Resultados")
        
        col_res1, col_res2, col_res3, col_res4 = st.columns(4)
        
        with col_res1:
            st.metric("Total Procesados", len(resultados))
        
        with col_res2:
            exitosos = sum(1 for r in resultados if r.precio > 0)
            st.metric("Precios Encontrados", exitosos)
        
        with col_res3:
            no_disponibles = sum(1 for r in resultados if r.precio == 0 and not r.error)
            st.metric("No Disponibles", no_disponibles)
        
        with col_res4:
            con_error = sum(1 for r in resultados if r.error)
            st.metric("Errores", con_error)
        
        # Tabla completa de resultados
        if resultados_tabla:
            st.subheader("Tabla Completa de Resultados")
            st.dataframe(resultados_tabla, use_container_width=True, hide_index=True)
            
            # Bot√≥n de descarga
            import pandas as pd
            df = pd.DataFrame(resultados_tabla)
            csv = df.to_csv(index=False)
            st.download_button(
                label="üì• Descargar Resultados (CSV)",
                data=csv,
                file_name=f"scraping_{establecimiento_seleccionado}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv"
            )
        
    except Exception as e:
        contenedor_estado.error(f"‚ùå Error durante el scraping: {e}")
        st.exception(e)

else:
    st.info("üëÜ Configura los par√°metros y haz clic en 'INICIAR MONITOREO' para comenzar.")
    
    # Mostrar informaci√≥n √∫til
    with st.expander("‚ÑπÔ∏è C√≥mo funciona el scraping"):
        st.markdown("""
        ### Proceso de Scraping:
        
        1. **L√≥gica de 48h**: Solo se actualizar√°n datos con m√°s de 48 horas de antig√ºedad
        2. **B√∫squeda Inteligente**: Intenta 3, 2, y 1 noche(s) hasta encontrar disponibilidad
        3. **Rate Limiting**: Esperas aleatorias (3-8s) entre peticiones para evitar bloqueos
        4. **Retry Logic**: Si falla, reintenta hasta 3 veces con exponential backoff
        5. **Anti-Detecci√≥n**: Navegador configurado con modo stealth
        
        ### Interpretaci√≥n de Resultados:
        
        - **Precio > $0**: Disponible al precio mostrado
        - **Precio = $0 (No disponible)**: No hay disponibilidad (ocupado)
        - **Error**: Problema t√©cnico (CAPTCHA, timeout, etc.)
        """)
